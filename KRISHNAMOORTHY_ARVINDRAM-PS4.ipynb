{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# INFO371: Problem Set 4, due May 22 at 3:30pm.\n",
    "\n",
    "### Before You Start\n",
    "\n",
    "This is a fun but challenging problem set. It will test your python skills, as well as your understanding of the material in class and in the readings. Start early and debug often! Some notes:\n",
    "\n",
    "* Part 1 is meant to be easy, so get through it quickly.\n",
    "* Part 2 (especially 2.1) will be difficult, but it is the lynchpin of\n",
    "  this problem set to make sure to do it well and understand what\n",
    "  you've done. If you find your gradient descent algorithm is taking\n",
    "  more than a few minutes to complete (it takes ~15s for me), debug more, compare notes with others, and go to the TA sessions.\n",
    "* Depending on how well you've done 2.1, parts 2.3 and 3.3 will be relatively painless or incredibly painful. \n",
    "* Part 3 and 4 (especially 4.3) will be computationally intensive. Don't leave this until the last minute, otherwise your code might be running when the deadline arrives.\n",
    "* Do the extra credit problems last."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Introduction to the assignment\n",
    "\n",
    "As with the last assignment, you will be using the [Boston Housing Prices Data Set](http://archive.ics.uci.edu/ml/datasets/Housing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline  \n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import model_selection\n",
    "# use cross_validation for earlier versions of sklearn\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "bdata = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Part 1: Getting oriented\n",
    "\n",
    "Learn the existing libraries for OLS and cross validation.\n",
    "\n",
    "### 1.1 Use existing libraries\n",
    "\n",
    "Soon, you will write your own gradient descent algorithm, which you will then use to minimize the squared error cost function.  First, however, let's use the canned versions that come with Python, to make sure we understand what we're aiming to achieve.\n",
    "\n",
    "Using the same Boston housing prices dataset as we did for the\n",
    "previous PS, use linear regression from sklearn or SciPy to explore\n",
    "the relationship between  median housing price and number of rooms per\n",
    "house.  Do this by first (a) regressing the housing price on the number of rooms per house, and then (b) regressing the housing price on the number of rooms per house and the (number of rooms per house) squared.  **Interpret your results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinregressResult(slope=9.102108981180308, intercept=-34.670620776438554, rvalue=0.69535994707153936, pvalue=2.4872288710078101e-74, stderr=0.41902656012134021)\n",
      "\n",
      "[-22.64326237   2.47012384]\n",
      "66.0588474848\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "boston = pd.DataFrame(bdata.data)\n",
    "boston.columns = bdata.feature_names\n",
    "boston['MEDV'] = bdata.target\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = sp.stats.linregress(boston['RM'].values, boston['MEDV'])\n",
    "print(sp.stats.linregress(boston['RM'].values, boston['MEDV']))\n",
    "print()\n",
    "\n",
    "secondSet = [boston[\"RM\"], boston[\"RM\"] * boston[\"RM\"]]\n",
    "linReg = lm.LinearRegression()\n",
    "x = np.array(secondSet).T\n",
    "y = np.array(boston[\"MEDV\"]).T\n",
    "solution = linReg.fit(x,y)\n",
    "betas = solution.coef_\n",
    "intercept = solution.intercept_\n",
    "\n",
    "print(betas)\n",
    "print(intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "your interpretation..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 1.2 Training and testing\n",
    "\n",
    "Above, you probably used all of your data to fit the regression line. In some circumstances this is a reasonable thing to do, but often this will result in overfitting. Let's redo the above results the ML way, using careful cross-validation.  Since you are now experts in cross-validation, and have written your own cross-validation algorithm from scratch, you can now take a shortcut and use the libraries that others have built for you.\n",
    "\n",
    "Using the cross\\_validation (or model\\_selection) functions from\n",
    "scikit-learn, use k-fold cross-validation to fit regression (a) above,\n",
    "i.e. the linear fit of housing price on number of rooms per house.\n",
    "Use a large $k$ value, say, 100.  Each fold of cross-validation will give you one slope coefficient and one intercept coefficient.  Plot the distribution of the k slope coefficients, and draw a vertical line at the value of the slope coefficient that you estimated in 1.1 using the full dataset.  What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEACAYAAABMEua6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkhpiqEtSQwx1SWqIoS5JDfl/dQOMDbJqZM4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f628a1d5fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## code\n",
    "kf = KFold(n_splits=100)\n",
    "X = boston[\"RM\"]\n",
    "Y = boston[\"MEDV\"]\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "KCoefs = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    kFoldsLinReg = lm.LinearRegression()\n",
    "    \n",
    "    output = kFoldsLinReg.fit(np.matrix(X_train).T, np.matrix(y_train).T)\n",
    "    #print('coefs',output.coef_)\n",
    "    predictions = output.predict(np.matrix(X_test).T)\n",
    "    KCoefs.append(output.coef_[0][0])\n",
    "\n",
    "plt.axvline(slope, color = 'r', linestyle = 'dashed', linewidth = 4)\n",
    "plt.hist(KCoefs, color = 'g', edgecolor = \"none\",)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "what do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 1.3 Regression lines\n",
    "\n",
    "Create a scatterplot of housing price on rooms per house, and add the\n",
    "two regression lines from 1.1 (or 1.2 if you prefer to do so). Show\n",
    "the linear regression line in red, and the linear + quadratic regression line (which may have curvature) in blue.  Label these two regression lines with the corresponding regression equations (i.e. the slope and intercept of the line).\n",
    "\n",
    "Bonus: Add the 95% confidence bands (i.e.,  the area that has a 95% chance of containing the true regression line) to each of these lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_first' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-194fd61e8030>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"True Median Housing Price\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinReg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_first\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinestyle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'dashed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinReg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_total\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'blue'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinestyle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'dashed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'g'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medgecolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"none\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_first' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEZCAYAAAC0HgObAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAtZonZZUtxApAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f62871d5ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# code\n",
    "p, ax = plt.subplots()\n",
    "plt.scatter(X,Y)\n",
    "plt.title(\"Unnormalized True vs. Predicted Median Housing Price in $1000\")\n",
    "plt.ylabel(\"Predicted Median Housing Price in $1000\")\n",
    "plt.xlabel(\"True Median Housing Price\")\n",
    "\n",
    "plt.plot(X, linReg.predict(X_first),color = 'r', linestyle = 'dashed', linewidth = 2)\n",
    "plt.plot(X, linReg.predict(X_total),color = 'blue', linestyle = 'dashed', linewidth = 2)\n",
    "plt.scatter(X, Y, color = 'g', edgecolor = \"none\",)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Part 2: Gradient descent: Linear Regression\n",
    "\n",
    "Here we'll really get serious.  It's your turn to implement some\n",
    "methods now.\n",
    "\n",
    "### 2.1 Implement gradient descent with one independent variable (Average rooms per house)\n",
    "\n",
    "Implement the basic gradient descent algorithm that we discussed in\n",
    "class. Use the version you implement to regress the housing price on\n",
    "the number of rooms per house. Experiment with 3-4 different values of\n",
    "the learning rate $R$ to find the one that works well.  Comment your results.\n",
    "\n",
    "Do the following:\n",
    "\n",
    "* At each beta value compute the gradient of the loss function (Note:\n",
    "  this is a vector of 2 components, bias and RM) and change beta in that direction.\n",
    "* Repeat the process until convergence (or until you lose the hope you\n",
    "  will ever get that far ;-)\n",
    "* Report the values of the parameter that minimize the loss function\n",
    "* For each value of $R$ Report the number of iterations it takes for\n",
    "  your algorithm to converge (or not to explode)\n",
    "* Report the total running time of your algorithm\n",
    "\n",
    "Here you are supposed to code the algorithm for a single explanatory\n",
    "variable (RM) and constant.  I strongly recommend you to do it in\n",
    "matrix form but you can also loop over your data.  Note that if done\n",
    "in matrix form, the function here is almost exactly the same as for\n",
    "multple regression you are asked to implement later.\n",
    "\n",
    "* Hint 1: Don't forget to implement stopping conditions, so that at\n",
    "  every iteration you check whether your results have\n",
    "  converged. Common approaches to this are to (a) check if the loss\n",
    "  has stopped decreasing; (b) check if both your current parameter\n",
    "  esimates are close to the estimates from the previous iteration; (c)\n",
    "  check if your gradient is close to zero..  In both cases, \"close\"\n",
    "  should not be == 0, it should be <=epsilon, where epsilon is a small\n",
    "  number, such as 1e-6).\n",
    "* Hint 2: include a maximum number of iterations to avoid the code\n",
    "  running for ever.  Report \"no convergence\" if this is exceeded.\n",
    "  Depending on your code, you may need 30,000 iterations.\n",
    "* Hint 3: $R = 10^{-4}$ may be a good starting point.  But it depends\n",
    "on how exactly do you write your loss function\n",
    "\n",
    "Bonus challenge if you feel for it:\n",
    "* Implement an adaptive version of learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(matrix([[ 0.46699831],\n",
      "        [ 3.10864868]]), 1000, False)\n",
      "Total Run Time of RM on MEDV; R = Default; MaxIterations = Default: 0.23 seconds.\n",
      "\n",
      "(matrix([[-24.08470445],\n",
      "        [  7.43794927]]), 20000, False)\n",
      "Total Run Time of RM on MEDV; R = 0.01; MaxIterations = 20000: 2.82 seconds.\n",
      "\n",
      "(matrix([[-34.66895811],\n",
      "        [  9.1018476 ]]), 33134, True)\n",
      "Total Run Time of RM on MEDV; R = 0.05; MaxIterations = 40000: 4.4 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "ols\n",
    "    Gradient Decent to minimize OLS. Used to find co-efficients of bivariate OLS Linear regression\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "xvalues, yvalues : narray\n",
    "    xvalues: n x 2, independent variables.  One column should be constant\n",
    "    yvalues: n x 1, dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "MaxIterations: Int\n",
    "    maximum number of iterations\n",
    "\n",
    "Returns\n",
    "-------\n",
    "(beta, iterations, convergence)\n",
    "beta: float vector, the parameter value \n",
    "iterations: integer, number of iterations\n",
    "convergece: logical, did the model converge?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#Adapted from http://ozzieliu.com/2016/02/09/gradient-descent-tutorial/\n",
    "def ols(xvalues, yvalues, R=0.0001, MaxIterations=1000):\n",
    "    ## set initial beta.  Beta=0 may be a good choice\n",
    "    beta = np.asmatrix(np.zeros((2, 1)))\n",
    "    iterations = 0\n",
    "    convergence = False\n",
    "    \n",
    "    m = xvalues.size\n",
    "    lossHistory = np.array([0.0] * MaxIterations)\n",
    "    ## iterate many times\n",
    "    while not convergence:\n",
    "        ### calculate loss\n",
    "        loss = calculateLoss(xvalues, yvalues, beta)\n",
    "        lossHistory[iterations] = loss\n",
    "    \n",
    "        ### calculate gradient of loss\n",
    "        gradient = (np.dot( xvalues.T, (  np.dot(   xvalues, beta   ) - yvalues  ) ) / m)\n",
    "        \n",
    "        ### calculate new_beta = beta - R*gradient\n",
    "        beta = beta - R * gradient\n",
    "        iterations = iterations + 1\n",
    "        \n",
    "        convergence = (np.all(abs(gradient) < 0.00001))\n",
    "        ### did L change much?  Did beta change much?  Have you tried too\n",
    "        ### many times?\n",
    "        if (iterations == MaxIterations):\n",
    "            return beta, iterations, convergence\n",
    "        \n",
    "        ### If converged:\n",
    "        if(convergence):\n",
    "            return beta, iterations, True\n",
    "    return beta, iterations, convergence\n",
    "\n",
    "\n",
    "def calculateLoss(X, y, beta):\n",
    "    \"\"\"\n",
    "    cost_function(X, y, beta) computes the cost of using beta as the\n",
    "    parameter for linear regression to fit the data points in X and y\n",
    "    \"\"\"\n",
    "    ## number of training examples\n",
    "    m = len(X)\n",
    "\n",
    "    ## Calculate the cost with the given parameters\n",
    "    J = np.sum(np.square( np.dot(  X, beta  ) - y )) / (2 * m)\n",
    "\n",
    "    return J\n",
    "\n",
    "XNew = np.column_stack((np.ones(506), X))\n",
    "YNew = np.asmatrix(Y).T\n",
    "\n",
    "# Test 1 and Time\n",
    "start_time = time.time()\n",
    "print(ols(XNew, YNew))\n",
    "print(\"Total Run Time of RM on MEDV; R = Default; MaxIterations = Default: \" + str(round(time.time() - start_time, 2)) + \" seconds.\")\n",
    "print()\n",
    "\n",
    "# Test 2 and Time\n",
    "start_time = time.time()\n",
    "print(ols(XNew, YNew, 0.01,20000))\n",
    "print(\"Total Run Time of RM on MEDV; R = 0.01; MaxIterations = 20000: \" + str(round(time.time() - start_time, 2)) + \" seconds.\")\n",
    "print()\n",
    "\n",
    "\n",
    "# Test 3 and Time\n",
    "start_time = time.time()\n",
    "print(ols(XNew, YNew, 0.05, 40000))\n",
    "print(\"Total Run Time of RM on MEDV; R = 0.05; MaxIterations = 40000: \" + str(round(time.time() - start_time, 2)) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Comment your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 2.2 Data normalization\n",
    "\n",
    "Soon, you will implement a version of gradient descent that can use an\n",
    "arbitrary number of independent variables. Before doing this, however,\n",
    "you should re-scale your features to ensure that no single feature\n",
    "dominates the cost function. Write a simple function to standardize\n",
    "your features.  Below is my version, but you can also use what you did\n",
    "in PS3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def standardize(raw_data):\n",
    "    return ((raw_data - np.mean(raw_data, axis = 0)) / np.std(raw_data, axis = 0))\n",
    "\n",
    "def de_standardize(data):\n",
    "    return data*np.std(data, axis = 0) + np.mean(data, axis = 0)\n",
    "\n",
    "def normalize(toNormalize):\n",
    "    mu = np.mean(toNormalize)\n",
    "    sigma = np.std(toNormalize)\n",
    "    return np.array([((x - mu) / sigma) for x in toNormalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 2.3 Implement gradient descent with an arbitrary number of independent variables\n",
    "\n",
    "Now that you have a simple version of gradient descent working, create\n",
    "a version of gradient descent that can take more than one independent\n",
    "variable.  Assume all independent variables will be continuous.  Test\n",
    "your algorithm using CRIM and RM as independent variables. Standardize\n",
    "these variables before before inputting them to the gradient descent\n",
    "algorithm.   If you used a pure matrix approach above, you can copy\n",
    "your previous algorithm almost verbatim.\n",
    "\n",
    "As before,  report and interpret your estimated coefficients, the number of iterations before convergence, and the total running time of your algorithm. Experiment with 2-3 different values of R.\n",
    "\n",
    "* *Hint 1: Be careful to implement this efficiently, otherwise it\n",
    "   might take a long time for your code to run.  Avoid looping over\n",
    "   matrix rows and columns if you can and use canned matrix\n",
    "   multiplication/other np functions like `np.mean`.  See the\n",
    "   normalization example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[  3.52430718e-15],\n",
      "       [  6.19318652e-01],\n",
      "       [ -2.51884444e-01]]), 1000, False)\n",
      "Total Run Time of RM and CRIM on MEDV; R = Default; MaxIterations = Default: 0.14 seconds.\n",
      "\n",
      "(array([[  4.43486369e-15],\n",
      "       [  6.40310186e-01],\n",
      "       [ -2.45696333e-01]]), 20000, False)\n",
      "Total Run Time of RM and CRIM on MEDV; R = 0.01; MaxIterations = 20000: 3.17 seconds.\n",
      "\n",
      "(array([[  8.38320556e-16],\n",
      "       [  4.84297613e-01],\n",
      "       [ -2.27734972e-01]]), 40000, False)\n",
      "Total Run Time of RM and CRIM on MEDV; R = 0.0001; MaxIterations = 40000: 5.52 seconds.\n",
      "\n",
      "(array([[  4.50728607e-15],\n",
      "       [  6.41496209e-01],\n",
      "       [ -2.44768962e-01]]), 657, True)\n",
      "Total Run Time of RM and CRIM on MEDV; R = 0.05; MaxIterations = 40000: 0.09 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "multivariate_ols\n",
    "    Gradient Decent to minimize OLS. Used to find co-efficients of bivariate OLS Linear regression\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "xvalue_matrix, yvalues : narray\n",
    "    xvalue_matrix: independent variable\n",
    "    yvalues: dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "MaxIterations: Int\n",
    "    maximum number of iterations\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "alpha: float\n",
    "    intercept\n",
    "    \n",
    "beta_array: array[float]\n",
    "    co-efficient\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def multiple_ols(xvalue_matrix, y, independentVariablesCount, R=0.01, MaxIterations=1000):\n",
    "    ## the algorithm is essentially the same as above\n",
    "    ## set initial beta.  Beta=0 may be a good choice\n",
    "    beta = np.asmatrix(np.zeros((independentVariablesCount, 1)))\n",
    "    iterations = 0\n",
    "    convergence = False\n",
    "    \n",
    "    m = xvalue_matrix.size\n",
    "    lossHistory = np.array([0.0] * MaxIterations)\n",
    "    ## iterate many times\n",
    "    while not convergence:\n",
    "        ### calculate loss\n",
    "        loss = calculateLoss(xvalue_matrix, y, beta)\n",
    "        lossHistory[iterations] = loss\n",
    "    \n",
    "        ### calculate gradient of loss\n",
    "        gradient = (np.dot( xvalue_matrix.T, (  np.dot(   xvalue_matrix, beta   ) - y  ) ) / m)\n",
    "        \n",
    "        ### calculate new_beta = beta - R*gradient\n",
    "        beta = beta - R * gradient\n",
    "        iterations = iterations + 1\n",
    "        \n",
    "        convergence = (np.all(abs(gradient) < 0.00001))\n",
    "        ### did L change much?  Did beta change much?  Have you tried too\n",
    "        ### many times?\n",
    "        if (iterations == MaxIterations):\n",
    "            return np.array(beta), iterations, convergence\n",
    "        \n",
    "        ### If converged:\n",
    "        if(convergence):\n",
    "            return np.array(beta), iterations, True\n",
    "    return np.array(beta), iterations, convergence\n",
    "\n",
    "\n",
    "def calculateLoss(X, y, beta):\n",
    "    \"\"\"\n",
    "    cost_function(X, y, beta) computes the cost of using beta as the\n",
    "    parameter for linear regression to fit the data points in X and y\n",
    "    \"\"\"\n",
    "    ## number of training examples\n",
    "    m = len(X)\n",
    "\n",
    "    ## Calculate the cost with the given parameters\n",
    "    \n",
    "    J = np.sum(np.square( np.dot(  X, beta  ) - y )) / (2 * m)\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "XMulti = np.column_stack((np.ones(506), standardize(X), standardize(boston[\"CRIM\"])))\n",
    "YMulti = np.asmatrix(standardize(Y)).T\n",
    "\n",
    "indepVarCount = XMulti[0].size\n",
    "\n",
    "# Test 1 and Time\n",
    "start_time = time.time()\n",
    "print(multiple_ols(XMulti, YMulti, indepVarCount))\n",
    "print(\"Total Run Time of RM and CRIM on MEDV; R = Default; MaxIterations = Default: \" + str(round(time.time() - start_time, 2)) + \" seconds.\")\n",
    "print()\n",
    "\n",
    "# Test 2 and Time\n",
    "start_time = time.time()\n",
    "print(multiple_ols(XMulti, YMulti, indepVarCount, 0.001, 20000))\n",
    "print(\"Total Run Time of RM and CRIM on MEDV; R = 0.01; MaxIterations = 20000: \" + str(round(time.time() - start_time, 2)) + \" seconds.\")\n",
    "print()\n",
    "\n",
    "\n",
    "# Test 3 and Time\n",
    "start_time = time.time()\n",
    "print(multiple_ols(XMulti, YMulti, indepVarCount, 0.0001, 40000))\n",
    "print(\"Total Run Time of RM and CRIM on MEDV; R = 0.0001; MaxIterations = 40000: \" + str(round(time.time() - start_time, 2)) + \" seconds.\")\n",
    "print()\n",
    "\n",
    "# Test 4 and Time\n",
    "start_time = time.time()\n",
    "print(multiple_ols(XMulti, YMulti, indepVarCount, 0.05, 40000))\n",
    "print(\"Total Run Time of RM and CRIM on MEDV; R = 0.05; MaxIterations = 40000: \" + str(round(time.time() - start_time, 2)) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Your observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 2.4 Compare standardized vs. non-standardized results\n",
    "\n",
    "Repeat the analysis from 2.3, but this time do not standardize your variables - i.e., use the original data. Use the same three values of R (0.1, 0.01, and 0.001). What do you notice about the running time and convergence properties of your algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[-0.51333217],\n",
      "       [ 3.92308633],\n",
      "       [-0.35798312]]), 1000, False)\n",
      "Total Run Time of NONSTANDARDIZED RM and CRIM on MEDV; R = Default; MaxIterations = Default: 0.17 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XMultiNonstandardized = np.column_stack((np.ones(506), X, boston[\"CRIM\"]))\n",
    "YMultiNonstandardized = np.asmatrix(Y).T\n",
    "\n",
    "indepVarCount = XMultiNonstandardized[0].size\n",
    "\n",
    "# Test 1 and Time\n",
    "start_time = time.time()\n",
    "print(multiple_ols(XMultiNonstandardized, YMultiNonstandardized, indepVarCount))\n",
    "print(\"Total Run Time of NONSTANDARDIZED RM and CRIM on MEDV; R = Default; MaxIterations = Default: \" + str(round(time.time() - start_time, 2)) + \" seconds.\")\n",
    "print()\n",
    "\n",
    "# Test 2 and Time\n",
    "start_time = time.time()\n",
    "print(multiple_ols(XMultiNonstandardized, YMultiNonstandardized, indepVarCount, 0.001, 20000))\n",
    "print(\"Total Run Time of NONSTANDARDIZED RM and CRIM on MEDV; R = 0.01; MaxIterations = 20000: \" + str(round(time.time() - start_time, 2)) + \" seconds.\")\n",
    "print()\n",
    "\n",
    "\n",
    "# Test 3 and Time\n",
    "start_time = time.time()\n",
    "print(multiple_ols(XMultiNonstandardized, YMultiNonstandardized, indepVarCount, 0.0001, 40000))\n",
    "print(\"Total Run Time of NONSTANDARDIZED RM and CRIM on MEDV; R = 0.0001; MaxIterations = 40000: \" + str(round(time.time() - start_time, 2)) + \" seconds.\")\n",
    "print()\n",
    "\n",
    "# Test 4 and Time\n",
    "start_time = time.time()\n",
    "print(multiple_ols(XMultiNonstandardized, YMultiNonstandardized, indepVarCount, 0.05, 40000))\n",
    "print(\"Total Run Time of NONSTANDARDIZED RM and CRIM on MEDV; R = 0.05; MaxIterations = 40000: \" + str(round(time.time() - start_time, 2)) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "What do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 3. Prediction\n",
    "\n",
    "Let's use our fitted model to make predictions about housing prices. Since the focus is now on prediction rather than the interpretation of the coefficients, make sure to use the standardized version of your features in everything that follows.\n",
    "\n",
    "### 3.1 Predicted values and RMSE\n",
    "\n",
    "Let's figure out how accurate this predictive model turns out to\n",
    "be. Report the average 10-fold cross-validated RMSE, separately for\n",
    "the training data and for the testing data.  Feel freee to use 'sklearn's canned cross validation.\n",
    "\n",
    "What is your test RMSE?  How does it compare to the performance of your nearest neighbor algorithm from the last problem set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def compute_rmse(inputList):\n",
    "        squares = [x ** 2 for x in inputList]\n",
    "        return np.sqrt(reduce(lambda x, y: x + y, squares) / len(squares))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.62828008855\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "tenkf = KFold(n_splits=10)\n",
    "\n",
    "\n",
    "RMSEs = []\n",
    "kf = KFold(n_splits=10)\n",
    "for train_index, test_index in kf.split(XMulti):\n",
    "    X_train, X_test = XMulti[train_index], XMulti[test_index]\n",
    "    y_train, y_test = YMulti[train_index], YMulti[test_index]\n",
    "    \n",
    "    myModel = np.asarray(multiple_ols(X_train, y_train, X_train[0].size, 0.05, 40000))\n",
    "    coefs = np.matrix([myModel[0][i][0] for i in range(0,3)]).T\n",
    "    predictions = np.dot(X_test, coefs)\n",
    "    anRMSE = compute_rmse(y_test - predictions)\n",
    "    \n",
    "    RMSEs.append(anRMSE)\n",
    "#     rmses1.append(rmse1)\n",
    "print(np.mean(RMSEs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Extra Credit: Logistic Regression\n",
    "\n",
    "For extra credit, implement logistic regression using gradient\n",
    "descent. Create a new variable (EXPENSIVE) to indicate whether the\n",
    "median housing price is more than $40,000. Estimate a logistic\n",
    "regression model of EXPENSIVE on CHAS and RM.  Compute accuracy and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "How well did it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 4 Regularization \n",
    "\n",
    "### 4.1 Get prepped\n",
    "\n",
    "Step 1: For each of the continuous features F in the original dataset, create a standardized version F_1.  Now, create polynomials up to degree 6 of each F_1: the square of F_1 (call this F_2); the cube of F_1 (call this F_3); and so forth up to F_6. If you originally had *K* features, you should now have *6K* features (i.e., we're going to ignore the original unscaled features for the remainder of this problem).\n",
    "\n",
    "Step 2: For simplicity, generate a single training and testing set.  Randomly sample 66% of your data and call this the training set, and set aside the remaining 34% as your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# EVERYTHING BUT CHAS\n",
    "FList = list(bdata.feature_names)\n",
    "FList.remove('CHAS')\n",
    "\n",
    "FDimDictionary = {}\n",
    "\n",
    "for feature in FList:\n",
    "    for i in range(1, 7):\n",
    "        FDimDictionary[\"\" + feature + \"_\" + str(i)] = list(standardize(boston[feature]) ** i)\n",
    "\n",
    "columnNames = sorted(FDimDictionary.keys())\n",
    "\n",
    "values = []\n",
    "for name in columnNames:\n",
    "    values.append(FDimDictionary[name])\n",
    "    \n",
    "extraFs = pd.DataFrame(dict(zip(columnNames, values)))\n",
    "    \n",
    "seed = np.random.seed(seed=13579)\n",
    "polyX_train, polyX_test, polyY_train, polyY_test = train_test_split(extraFs, Y, test_size=0.34, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 4.2 Let's overfit!\n",
    "Now, using your version of multivariate regression from 2.3, (over)fit\n",
    "your model on the training data. Using your training set, regress\n",
    "housing price on as many of those *6K* features as you can.  If you\n",
    "get too greedy, or if you did not efficiently implement your solution\n",
    "to 2.3, it's possible this will take a long time to compute, or does\n",
    "not converge at all.  \n",
    "\n",
    "Report the RMSE when you apply your model to your training set and to your testing set. How do these numbers compare to each other, and to the RMSE from 2.5 and nearest neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 21.05021701],\n",
      "       [ -1.83437679],\n",
      "       [ -0.22534915],\n",
      "       [  0.33430524],\n",
      "       [ -0.05521778],\n",
      "       [ -0.44241847],\n",
      "       [ -3.43723996],\n",
      "       [  0.90357971],\n",
      "       [ -0.87759068],\n",
      "       [  0.4780555 ]]), 11169, True)\n",
      "Total Run Time of CRIM_1, AGE_3, ZN_2, INDUS_4, DIS_3, NOX_1, RAD_2, PTRATIO_3 and RM_3 on MEDV; R = 0.05; R = 40000: 1.32 seconds.\n",
      "(173, 72)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "# code\n",
    "OverFitX = np.column_stack((np.ones(len(polyX_train[\"CRIM_1\"])), polyX_train[\"CRIM_1\"], polyX_train[\"AGE_3\"], polyX_train[\"ZN_2\"], polyX_train[\"INDUS_4\"], polyX_train[\"DIS_3\"], polyX_train[\"NOX_1\"], polyX_train[\"RAD_2\"], polyX_train[\"PTRATIO_3\"], polyX_train[\"RM_3\"]))\n",
    "polyY_train = np.asmatrix(polyY_train).T\n",
    "indepVarCount = OverFitX[0].size\n",
    "\n",
    "start_time = time.time()\n",
    "print(multiple_ols(OverFitX, polyY_train, indepVarCount, 0.05, 40000))\n",
    "print(\"Total Run Time of CRIM_1, AGE_3, ZN_2, INDUS_4, DIS_3, NOX_1, RAD_2, PTRATIO_3 and RM_3 on MEDV; R = 0.05; R = 40000: \" + str(round(time.time() - start_time, 2)) + \" seconds.\")\n",
    "\n",
    "myModel = np.asarray(multiple_ols(OverFitX, polyY_train, indepVarCount, 0.05, 40000))\n",
    "coefs = np.matrix([myModel[0][i][0] for i in range(0,indepVarCount)]).T\n",
    "\n",
    "print(polyX_test.shape)\n",
    "print(coefs.shape)\n",
    "\n",
    "# predictions = np.dot(polyX_test, coefs)\n",
    "# anRMSE = compute_rmse(polyY_test - predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Using our \"overfit\" regression, we see a drop in RMSE. As above, the actual values will depend on the random splits in the data, but in general we see that the RMSE is reduced by the largest margin on the training set, but the RMSE on the test set is still lower than the results from a regression with just two regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 4.3 Ridge regularization\n",
    "Incorporate L2 (Ridge) regularization into your multivariate_ols\n",
    "regression.  You have to take your multiple regression code, and add\n",
    "the penalization term to the gradient.  Add the penalty tuning parameter as an\n",
    "additional argument to this function.  Remember not to penalize the\n",
    "constant! \n",
    "\n",
    "Use your regularized regression to re-fit the model from 4.2 above on your training data. Try this for several different values of lambda, and report your RMSE for each lambda separately for your training and testing data. How do these numbers compare to each other, to the RMSE from 4.2,  to the RMSE from 2.3, and to the RMSE from nearest neighbors?\n",
    "\n",
    "Finally, go and brag to your friends about how you just implemented\n",
    "ridge-regularized multivariate regression using gradient descent\n",
    "optimization, from scratch. If you still have friends left ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(matrix([[  1.38628822e-06],\n",
      "        [ -2.22136989e-07],\n",
      "        [ -1.02364955e-06],\n",
      "        [  1.37589584e-06],\n",
      "        [  2.21184474e-06],\n",
      "        [  1.15009949e-06],\n",
      "        [ -2.12254161e-07],\n",
      "        [  1.13942271e-06],\n",
      "        [ -1.88515255e-06],\n",
      "        [  3.03056871e-06]]), 1202, False)\n",
      "Total Run Time of RIDGE REGULARIZED CRIM_1, AGE_3, ZN_2, INDUS_4, DIS_3, NOX_1, RAD_2, PTRATIO_3 and RM_3 on MEDV; R = 0.05; R = 40000: 0.23 seconds.\n"
     ]
    }
   ],
   "source": [
    "def multiple_regularized_ols(xvalue_matrix, y, independentVariablesCount, R=0.01, MaxIterations=1000, lbd=1):\n",
    "    ## the only difference with multiple OLS: add the penalty\n",
    "    ## parameter (both to loss and to gradient)\n",
    "    W = np.eye(independentVariablesCount)\n",
    "    W[0][0] = 0\n",
    "    \n",
    "    beta = np.asmatrix(np.zeros((independentVariablesCount, 1)))\n",
    "    iterations = 0\n",
    "    convergence = False\n",
    "    \n",
    "    m = xvalue_matrix.size\n",
    "    lossHistory = np.array([0.0] * MaxIterations)\n",
    "    \n",
    "    ## iterate many times\n",
    "    while not convergence:\n",
    "        ### calculate loss\n",
    "        pOfBeta = np.array(np.dot(beta.T, np.dot(W, beta)))[0][0]\n",
    "        loss = (calculateLoss(xvalue_matrix, y, beta) + (lbd * pOfBeta))\n",
    "        lossHistory[iterations] = loss\n",
    "    \n",
    "        ### calculate gradient of loss\n",
    "        gradient = (( np.dot(  xvalue_matrix.T, (   np.dot(    xvalue_matrix, beta    ) - y   )  ) / m ) + (lbd * pOfBeta))\n",
    "        \n",
    "        ### calculate new_beta = beta - R*gradient\n",
    "        beta = beta - R * gradient\n",
    "        iterations = iterations + 1\n",
    "        \n",
    "        convergence = (np.all(abs(gradient) < 0.001))\n",
    "        ### did L change much?  Did beta change much?  Have you tried too\n",
    "        ### many times?\n",
    "        if (iterations > 1 and lossHistory[iterations - 1] < loss):\n",
    "            return beta, iterations, convergence\n",
    "        \n",
    "        if (iterations == MaxIterations):\n",
    "            return beta, iterations, convergence\n",
    "        \n",
    "        ### If converged:\n",
    "        if(convergence):\n",
    "            return beta, iterations, True\n",
    "    return alpha, beta_array, iterations, convergence\n",
    "\n",
    "\n",
    "def calculateLoss(X, y, beta):\n",
    "    \"\"\"\n",
    "    cost_function(X, y, beta) computes the cost of using beta as the\n",
    "    parameter for linear regression to fit the data points in X and y\n",
    "    \"\"\"\n",
    "    ## number of training examples\n",
    "    m = len(X)\n",
    "\n",
    "    ## Calculate the cost with the given parameters\n",
    "    J = np.sum(np.square( np.dot(  X, beta  ) - y )) / (2 * m)\n",
    "    return J\n",
    "\n",
    "start_time = time.time()\n",
    "print(multiple_regularized_ols(OverFitX, polyY_train, indepVarCount, 0.0000000005, 1202, 1300))\n",
    "print(\"Total Run Time of RIDGE REGULARIZED CRIM_1, AGE_3, ZN_2, INDUS_4, DIS_3, NOX_1, RAD_2, PTRATIO_3 and RM_3 on MEDV; R = 0.05; R = 40000: \" + str(round(time.time() - start_time, 2)) + \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "what do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Extra Credit 2: Cross-validate lambda\n",
    "\n",
    "Use k-fold cross-validation to select the optimal value of lambda. Report the average RMSE across all training sets, and the average RMSE across all testing sets. How do these numbers compare to each other, to the RMSE from your previous efforts?  Finally, create a scatter plot that shows RMSE as a function of lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "What is your optimal lambda?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### (Showoff) Extra Credit 3: Lambda and coefficients\n",
    "\n",
    "If you're feeling extra-special, create a parameter plot that shows how the different coefficient estimates change as a function of lambda. To make this graph intelligible, only include the *K* original F_s features in this plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "What do you see with coefficients and lambda?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "INFO371-PS4.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
